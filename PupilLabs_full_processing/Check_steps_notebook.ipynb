{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06022cc",
   "metadata": {},
   "source": [
    "# Extracting paths and group numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3927bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "folder_path = \"/home/kaibald231/ABC/26Apr\" #26Apr #28Apr #28jan #27jan\n",
    "if os.path.isdir(folder_path):\n",
    "    print(\"Folder exists\")\n",
    "else:\n",
    "    print(\"Folder does not exist\")\n",
    "    s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26f18bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14501/3091604451.py:172: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['001' '002' '003' '004' '005' '006' '007' '008' '009' '010' '011' '012'\n",
      " '013' '014' '015' '016' '017' '018' '019' '020' '021' '022' '023' '024'\n",
      " '025' '026' '027' '028' '029' '030' '031' '032' '033' '034' '035' '036'\n",
      " '037' '038' '039' '040' '041' '042' '043' '044' '045' '046' '047' '048'\n",
      " '049' '050' '051' '052' '053' '054' '055' '056' '057' '058' '059' '060'\n",
      " '061' '062' '063' '064' '065' '066' '067' '068' '069' '070' '071' '072'\n",
      " '073' '074' '075' '076' '077' '078' '079' '080' '081' '082' '083' '084'\n",
      " '085' '086' '087' '088' '089' '090' '091' '092' '093' '094' '095' '096'\n",
      " '097' '098' '099' '100' '101' '102' '103' '104' '105' '106' '107' '108'\n",
      " '109' '110' '111' '112' '113' '114' '115' '116' '117' '118' '119' '120'\n",
      " '121' '122' '123' '124' '125' '126' '127' '128' '129' '130' '131' '132'\n",
      " '133' '134' '135' '136' '137' '138' '139' '140' '141' '142' '143' '144'\n",
      " '145' '146' '147' '148' '149' '150' '151' '152' '153' '154' '155' '156'\n",
      " '157' '158' '159' '160' '161' '162' '163' '164' '165' '166' '167' '168'\n",
      " '169' '170' '171' '172' '173' '174' '175' '176' '177' '178' '179' '180'\n",
      " '181' '182' '183' '184' '185' '186' '187' '188' '189' '190' '191' '192'\n",
      " '193' '194' '195' '196' '197' '198' '199']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, ('phase1', 'part number')] = df['phase1']['part number'].astype(str).str.zfill(3)\n",
      "/tmp/ipykernel_14501/3091604451.py:173: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['001' '002' '003' '004' '005' '006' '007' '008' '009' '010' '011' '012'\n",
      " '013' '014' '015' '016' '017' '018' '019' '020' '021' '022' '023' '024'\n",
      " '025' '026' '027' '028' '029' '030' '031' '032' '033' '034' '035' '036'\n",
      " '037' '038' '039' '040' '041' '042' '043' '044' '045' '046' '047' '048'\n",
      " '049' '050' '051' '052' '053' '054' '055' '056' '057' '058' '059' '060'\n",
      " '061' '062' '063' '064' '065' '066' '067' '068' '069' '070' '071' '072'\n",
      " '073' '074' '075' '076' '077' '078' '079' '080' '081' '082' '083' '084'\n",
      " '085' '086' '087' '088' '089' '090' '091' '092' '093' '094' '095' '096'\n",
      " '097' '098' '099' '100' '101' '102' '103' '104' '105' '106' '107' '108'\n",
      " '109' '110' '111' '112' '113' '114' '115' '116' '117' '118' '119' '120'\n",
      " '121' '122' '123' '124' '125' '126' '127' '128' '129' '130' '131' '132'\n",
      " '133' '134' '135' '136' '137' '138' '139' '140' '141' '142' '143' '144'\n",
      " '145' '146' '147' '148' '149' '150' '151' '152' '153' '154' '155' '156'\n",
      " '157' '158' '159' '160' '161' '162' '163' '164' '165' '166' '167' '168'\n",
      " '169' '170' '171' '172' '173' '174' '175' '176' '177' '178' '179' '180'\n",
      " '181' '182' '183' '184' '185' '186' '187' '188' '189' '190' '191' '192'\n",
      " '193' '194' '195' '196' '197' '198' '199']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, ('phase2', 'part number')] = df['phase2']['part number'].astype(str).str.zfill(3)\n",
      "/tmp/ipykernel_14501/3091604451.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].apply(parse_date)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['/home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN9:15/093/20250426101622479/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN10:15/063/20250426112258634/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN10:15/031/20250426112215382/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN13:15/037/20250426141354347/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN13:15/014/20250426142058230/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN14:15/129/20250426151921955/exports',\n",
       "  '2',\n",
       "  True],\n",
       " ['/home/kaibald231/ABC/26Apr/BIN14:15/158/20250426151952420/exports',\n",
       "  '2',\n",
       "  True]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def find_exports_folders(root_path):\n",
    "    \"\"\"\n",
    "    Recursively searches for all folders named 'exports' starting from root_path.\n",
    "    \n",
    "    Args:\n",
    "        root_path (str): The path to the root folder to search from\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of absolute paths to all folders named 'exports'\n",
    "    \"\"\"\n",
    "    exports_paths = []\n",
    "    \n",
    "    # Check if the root path exists\n",
    "    if not os.path.exists(root_path):\n",
    "        print(f\"Warning: Path '{root_path}' does not exist\")\n",
    "        return exports_paths\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        # Check if 'exports' is in the list of directory names\n",
    "        if 'exports' in dirs:\n",
    "            # Get the full path to the exports folder\n",
    "            exports_path = os.path.join(root, 'exports')\n",
    "            exports_paths.append(exports_path)\n",
    "    \n",
    "    return exports_paths\n",
    "\n",
    "def extract_part_number_and_date(paths):\n",
    "    records = []\n",
    "\n",
    "    for path in paths:\n",
    "        try:\n",
    "            parts = path.split(os.sep)\n",
    "            # Look for the timestamp\n",
    "            for i in range(len(parts)-1, 0, -1):\n",
    "                part = parts[i]\n",
    "                if part.isdigit() and len(part) >= 8:\n",
    "                    date_str = part[:8]  # YYYYMMDD\n",
    "                    mm_dd = f\"{date_str[4:6]}_{date_str[6:8]}\"\n",
    "                    part_number = parts[i - 1]  # Folder before timestamp\n",
    "                    records.append({'part_number': part_number, 'date': mm_dd})\n",
    "                    break\n",
    "            else:\n",
    "                records.append({'part_number': None, 'date': None})\n",
    "        except Exception:\n",
    "            records.append({'part_number': None, 'date': None})\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def restructure_phase_columns(df):\n",
    "    # Step 1: Clean column names\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "    # Step 2: Find the split index (where 'part number.1' appears)\n",
    "    split_idx = df.columns.get_loc('part number.1')\n",
    "\n",
    "    # Step 3: Split columns\n",
    "    phase1_cols = df.columns[:split_idx]\n",
    "    phase2_cols = df.columns[split_idx:]\n",
    "\n",
    "    # Step 4: Create MultiIndex columns\n",
    "    new_columns = []\n",
    "    for col in phase1_cols:\n",
    "        col_name = 'date' if col == 'phase1 date' else col\n",
    "        new_columns.append(('phase1', col_name))\n",
    "\n",
    "    for col in phase2_cols:\n",
    "        col = col.replace('.1', '').strip()\n",
    "        col_name = 'date' if col == 'phase2 date' else col\n",
    "        new_columns.append(('phase2', col_name))\n",
    "\n",
    "    # Step 5: Apply new MultiIndex columns\n",
    "    df.columns = pd.MultiIndex.from_tuples(new_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_date_column(df, col='date'):\n",
    "    month_map = {\n",
    "        'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04',\n",
    "        'may': '05', 'jun': '06', 'jul': '07', 'aug': '08',\n",
    "        'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'\n",
    "    }\n",
    "\n",
    "    def parse_date(val):\n",
    "        if pd.isna(val):\n",
    "            return None\n",
    "        val = str(val).lower().replace(\" \", \"\")  # e.g., 'Apr 26' -> 'apr26'\n",
    "        for mon, mm in month_map.items():\n",
    "            if val.startswith(mon):\n",
    "                day_part = val[len(mon):]\n",
    "                if day_part.isdigit():\n",
    "                    return f\"{mm}_{day_part.zfill(2)}\"  # zero-pad day\n",
    "        return None  # fallback if no match\n",
    "\n",
    "    df[col] = df[col].apply(parse_date)\n",
    "    return df\n",
    "\n",
    "def merge_group_tablo_order(small_df, big_df):\n",
    "    merged_rows = []\n",
    "\n",
    "    for phase in ['phase1', 'phase2']:\n",
    "        # Extract and rename necessary columns\n",
    "        df_phase = big_df[phase][['part number', 'date', 'group', 'tablo order']].copy()\n",
    "        df_phase.columns = ['part_number', 'date', 'group', 'tablo_order']\n",
    "        \n",
    "        # Merge with small_df\n",
    "        merged = pd.merge(\n",
    "            small_df,\n",
    "            df_phase,\n",
    "            on=['part_number', 'date'],\n",
    "            how='left'\n",
    "        )\n",
    "        merged['phase'] = phase  # optional: track source phase\n",
    "        merged_rows.append(merged)\n",
    "\n",
    "    # Combine phase1 + phase2 results (you can skip this if you only need matched rows)\n",
    "    final = pd.concat(merged_rows, ignore_index=True)\n",
    "\n",
    "    # Optionally drop rows with no match\n",
    "    final = final.dropna(subset=['group', 'tablo_order'], how='all')\n",
    "\n",
    "    return final\n",
    "\n",
    "def process_paths(df, paths):\n",
    "    result = []\n",
    "    \n",
    "    # Create a mapping from part_number to its corresponding row in df\n",
    "    part_to_row = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        part_to_row[row['part_number']] = row\n",
    "    \n",
    "    for path in paths:\n",
    "        # Extract part_number from the path (assuming it's the 2nd last directory)\n",
    "        part_number = path.split('/')[-3]\n",
    "        \n",
    "        # Get the corresponding row from df\n",
    "        row = part_to_row.get(part_number)\n",
    "        \n",
    "        if row is None:\n",
    "            # If part_number not found in df, skip or handle as needed\n",
    "            continue\n",
    "        \n",
    "        # Determine the value for the 2nd element\n",
    "        if row['phase'] == 'phase2':\n",
    "            value = '2'\n",
    "        elif row['phase'] == 'phase1':\n",
    "            value = row['group']\n",
    "        else:\n",
    "            value = None  # Fallback (adjust as needed)\n",
    "        \n",
    "        # Determine if tablo_order is \"bad\" (assuming NaN is not \"bad\")\n",
    "        is_valid = False if str(row['tablo_order']).lower() == \"bad\" else True\n",
    "        \n",
    "        # Append to result\n",
    "        result.append([path, value, is_valid])\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = find_exports_folders(folder_path)\n",
    "current_folders_df = extract_part_number_and_date(result)\n",
    "\n",
    "# https://docs.google.com/spreadsheets/d/14dm8AJ1-oXXnaGY_-iNnv1_S62QoHA3jy3gcePCKnF4/edit?usp=sharing\n",
    "sheet_id = \"14dm8AJ1-oXXnaGY_-iNnv1_S62QoHA3jy3gcePCKnF4\"\n",
    "sheet_name = \"Sheet1\"  # or whatever your tab is named\n",
    "csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "\n",
    "# Step 1: Read the raw two-level header\n",
    "df = pd.read_csv(csv_url)\n",
    "df = restructure_phase_columns(df)\n",
    "df.loc[:, ('phase1', 'part number')] = df['phase1']['part number'].astype(str).str.zfill(3)\n",
    "df.loc[:, ('phase2', 'part number')] = df['phase2']['part number'].astype(str).str.zfill(3)\n",
    "\n",
    "# Apply to all phase columns (e.g., 'phase1', 'phase2', etc.)\n",
    "for phase_col in ['phase1', 'phase2']:  # Add more if needed\n",
    "    if phase_col in df.columns:\n",
    "        # Ensure we're working with a copy to avoid SettingWithCopyWarning\n",
    "        df[phase_col] = df[phase_col].copy()\n",
    "        if 'date' in df[phase_col].columns:\n",
    "            df[phase_col] = normalize_date_column(df[phase_col], col='date')\n",
    "\n",
    "result_df = merge_group_tablo_order(current_folders_df, df)\n",
    "process_paths(result_df ,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799d0f0",
   "metadata": {},
   "source": [
    "# Checking if there are no missing files after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771402bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 subfolders with all required files:\n",
      "\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/000\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/001\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/002\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/003\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/004\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/005\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/006\n",
      "✅ /home/kaibald231/ABC/26Apr/BIN9:15/092/20250426101651949/exports/processed_2abc092/007\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 158\u001b[0m\n\u001b[1;32m    155\u001b[0m     missing_folders \u001b[38;5;241m=\u001b[39m check_files_in_subfolders(processed_folder)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Extract just the folder paths (not the missing files list)\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     folder_paths_only \u001b[38;5;241m=\u001b[39m [folder_path \u001b[38;5;28;01mfor\u001b[39;00m folder_path, _ \u001b[38;5;129;01min\u001b[39;00m missing_folders]\n\u001b[1;32m    159\u001b[0m     all_folders_with_missing_files\u001b[38;5;241m.\u001b[39mextend(folder_paths_only)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll folders with missing files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_folders_with_missing_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 158\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m     missing_folders \u001b[38;5;241m=\u001b[39m check_files_in_subfolders(processed_folder)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Extract just the folder paths (not the missing files list)\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     folder_paths_only \u001b[38;5;241m=\u001b[39m [folder_path \u001b[38;5;28;01mfor\u001b[39;00m folder_path, _ \u001b[38;5;129;01min\u001b[39;00m missing_folders]\n\u001b[1;32m    159\u001b[0m     all_folders_with_missing_files\u001b[38;5;241m.\u001b[39mextend(folder_paths_only)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll folders with missing files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_folders_with_missing_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_processed_folder(root_folder: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    Recursively search for the first folder starting with 'processed' under the given root folder.\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): The path to start searching from.\n",
    "\n",
    "    Returns:\n",
    "        Path or None: Path to the first folder starting with 'processed', or None if not found.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_folder)\n",
    "\n",
    "    for dirpath, dirnames, _ in os.walk(root_path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname.lower().startswith(\"processed_\"):\n",
    "                return Path(dirpath) / dirname\n",
    "    return None\n",
    "\n",
    "def check_files_in_subfolders(root_folder):\n",
    "    \"\"\"\n",
    "    Check if specific files are present in subfolders of the given root folder.\n",
    "    Reports which files are missing from which subfolders.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples (folder_path, missing_files) for folders with missing files\n",
    "    \"\"\"\n",
    "    required_files = [\n",
    "        \"gazeData_mapped.tsv\",\n",
    "        # \"fixations_mapped.tsv\", \n",
    "        # \"pupil_trimmed_audio.mp3\",\n",
    "        # \"pupil_segment_transcriptions.json\"\n",
    "    ]\n",
    "    \n",
    "    root_path = Path(root_folder)\n",
    "    \n",
    "    if not root_path.exists():\n",
    "        print(f\"Error: Root folder '{root_folder}' does not exist.\")\n",
    "        return []\n",
    "    \n",
    "    if not root_path.is_dir():\n",
    "        print(f\"Error: '{root_folder}' is not a directory.\")\n",
    "        return []\n",
    "    \n",
    "    # Get all subdirectories\n",
    "    subfolders = [d for d in root_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not subfolders:\n",
    "        print(f\"No subfolders found in '{root_folder}'.\")\n",
    "        return []\n",
    "    \n",
    "    # print(f\"Checking {len(subfolders)} subfolders in '{root_folder}'...\")\n",
    "    # print(\"-\" * 60)\n",
    "    \n",
    "    folders_with_missing_files = []\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        missing_files = []\n",
    "        \n",
    "        # Check each required file\n",
    "        for required_file in required_files:\n",
    "            file_path = subfolder / required_file\n",
    "            if not file_path.exists():\n",
    "                missing_files.append(required_file)\n",
    "        \n",
    "        # If any files are missing, record this subfolder\n",
    "        if missing_files:\n",
    "            folders_with_missing_files.append((str(subfolder), missing_files))\n",
    "    \n",
    "    # Report results\n",
    "    if folders_with_missing_files:\n",
    "        print(f\"Found {len(folders_with_missing_files)} subfolders with missing files:\\n\")\n",
    "        \n",
    "        for subfolder, missing_files in folders_with_missing_files:\n",
    "            print(f\"📁 {subfolder}\")\n",
    "            for missing_file in missing_files:\n",
    "                print(f\"   ❌ Missing: {missing_file}\")\n",
    "            print()\n",
    "    return folders_with_missing_files\n",
    "\n",
    "def check_files_in_subfolders(root_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    Check if specific files are present in subfolders of the given root folder.\n",
    "    Reports which subfolders contain all required files.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder to search in\n",
    "        \n",
    "    Returns:\n",
    "        list: List of folder paths that have all required files\n",
    "    \"\"\"\n",
    "    required_files = [\n",
    "        \"painting_in_world.mp4\",\n",
    "        \"ref_gaze.mp4\", \n",
    "        \"ref2world_mapping.mp4\",\n",
    "        # \"pupil_segment_transcriptions.json\"\n",
    "    ]\n",
    "    \n",
    "    root_path = Path(root_folder)\n",
    "    \n",
    "    if not root_path.exists():\n",
    "        print(f\"Error: Root folder '{root_folder}' does not exist.\")\n",
    "        return []\n",
    "    \n",
    "    if not root_path.is_dir():\n",
    "        print(f\"Error: '{root_folder}' is not a directory.\")\n",
    "        return []\n",
    "    \n",
    "    # Get all subdirectories\n",
    "    subfolders = [d for d in root_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not subfolders:\n",
    "        print(f\"No subfolders found in '{root_folder}'.\")\n",
    "        return []\n",
    "    \n",
    "    folders_with_all_files = []\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        has_all_files = True\n",
    "        \n",
    "        # Check each required file\n",
    "        for required_file in required_files:\n",
    "            file_path = subfolder / required_file\n",
    "            if not file_path.exists():\n",
    "                has_all_files = False\n",
    "                break\n",
    "        \n",
    "        # If all files are present, record this subfolder\n",
    "        if has_all_files:\n",
    "            folders_with_all_files.append(str(subfolder))\n",
    "    \n",
    "    # Report results\n",
    "    if folders_with_all_files:\n",
    "        print(f\"Found {len(folders_with_all_files)} subfolders with all required files:\\n\")\n",
    "        for subfolder in folders_with_all_files:\n",
    "            print(f\"✅ {subfolder}\")\n",
    "    else:\n",
    "        print(\"No subfolders contain all required files.\")\n",
    "    \n",
    "    return folders_with_all_files\n",
    "\n",
    "\n",
    "# Collect all folder paths with missing files\n",
    "all_folders_with_missing_files = []\n",
    "\n",
    "result = find_exports_folders(folder_path)\n",
    "for sub_folder in result:\n",
    "    processed_folder = find_processed_folder(sub_folder)\n",
    "    missing_folders = check_files_in_subfolders(processed_folder)\n",
    "    \n",
    "    # Extract just the folder paths (not the missing files list)\n",
    "    folder_paths_only = [folder_path for folder_path, _ in missing_folders]\n",
    "    all_folders_with_missing_files.extend(folder_paths_only)\n",
    "\n",
    "print(f\"\\nAll folders with missing files: {len(all_folders_with_missing_files)} total\")\n",
    "print(all_folders_with_missing_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64539202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384a0e7",
   "metadata": {},
   "source": [
    "# If there are missing data, run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc96f080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs = [\n",
      "    (\n",
      "        \"/home/kaibald231/ABC/27jan/BIN9:15/052/20250127100823065/exports/007/gaze_positions.csv\",\n",
      "        \"/home/kaibald231/ABC/27jan/BIN9:15/052/20250127100823065/exports/007/world.mp4\",\n",
      "        \"/home/kaibald231/ABC/27jan/BIN9:15/052/20250127100823065/exports/processed_2abc052/007/FLORIS Frans, Portrait de dame âgée, parfois dit la Femme du fauconnier Inv.47.jpg\",\n",
      "        \"/home/kaibald231/ABC/27jan/BIN9:15/052/20250127100823065/exports/processed_2abc052/007\",\n",
      "        \"True\"\n",
      "    ),\n",
      "    (\n",
      "        \"/home/kaibald231/ABC/27jan/BIN14:15/134/20250127150153661/exports/007/gaze_positions.csv\",\n",
      "        \"/home/kaibald231/ABC/27jan/BIN14:15/134/20250127150153661/exports/007/world.mp4\",\n",
      "        \"/home/kaibald231/ABC/27jan/BIN14:15/134/20250127150153661/exports/processed_2abc134/007/FLORIS Frans, Portrait de dame âgée, parfois dit la Femme du fauconnier Inv.47.jpg\",\n",
      "        \"/home/kaibald231/ABC/27jan/BIN14:15/134/20250127150153661/exports/processed_2abc134/007\",\n",
      "        \"True\"\n",
      "    )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def create_jobs_from_missing_folders(missing_folders):\n",
    "    \"\"\"\n",
    "    Create a jobs list from missing folder paths.\n",
    "    \n",
    "    Args:\n",
    "        missing_folders (list): List of folder paths that are missing required files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples in the format (gaze_csv, world_mp4, ref_jpg, output_path, \"False\")\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    for missing_folder in missing_folders:\n",
    "        try:\n",
    "            missing_path = Path(missing_folder)\n",
    "            \n",
    "            # Extract the folder name (e.g., \"006\" from the path)\n",
    "            folder_name = missing_path.name\n",
    "            \n",
    "            # Navigate to the exports folder\n",
    "            # From: /home/kaibald231/ABC/26Apr/BIN13:15/014/20250426142058230/exports/processed_2abc014/006\n",
    "            # To:   /home/kaibald231/ABC/26Apr/BIN13:15/014/20250426142058230/exports/006\n",
    "            \n",
    "            # Go up to exports folder and then to the folder with same name\n",
    "            exports_path = missing_path.parent.parent / folder_name\n",
    "            \n",
    "            # Path 1: gaze_positions.csv in exports folder\n",
    "            gaze_csv = exports_path / \"gaze_positions.csv\"\n",
    "            \n",
    "            # Path 2: Find .mp4 file in exports folder (usually world.mp4)\n",
    "            mp4_files = list(exports_path.glob(\"*.mp4\"))\n",
    "            if mp4_files:\n",
    "                world_mp4 = mp4_files[0]  # Take the first .mp4 file found\n",
    "            else:\n",
    "                # If no .mp4 found, use expected path\n",
    "                world_mp4 = exports_path / \"world.mp4\"\n",
    "            \n",
    "            # Path 3: Find .jpg file in the missing folder (original path)\n",
    "            jpg_files = list(missing_path.glob(\"*.jpg\"))\n",
    "            if jpg_files:\n",
    "                ref_jpg = jpg_files[0]  # Take the first .jpg file found\n",
    "            else:\n",
    "                # If no .jpg found, use a generic name\n",
    "                ref_jpg = missing_path / \"reference.jpg\"\n",
    "            \n",
    "            # Path 4: Output path (the missing folder itself)\n",
    "            output_path = missing_folder\n",
    "            \n",
    "            # Path 5: Always \"False\"\n",
    "            flag = \"True\"\n",
    "            \n",
    "            # Create the job tuple\n",
    "            job_tuple = (\n",
    "                str(gaze_csv),\n",
    "                str(world_mp4),\n",
    "                str(ref_jpg),\n",
    "                str(output_path),\n",
    "                flag\n",
    "            )\n",
    "            \n",
    "            jobs.append(job_tuple)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing folder {missing_folder}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "def print_jobs_list(jobs):\n",
    "    \"\"\"\n",
    "    Print the jobs list in a readable format.\n",
    "    \"\"\"\n",
    "    print(\"jobs = [\")\n",
    "    for i, job in enumerate(jobs):\n",
    "        print(\"    (\")\n",
    "        print(f'        \"{job[0]}\",')\n",
    "        print(f'        \"{job[1]}\",')\n",
    "        print(f'        \"{job[2]}\",')\n",
    "        print(f'        \"{job[3]}\",')\n",
    "        print(f'        \"{job[4]}\"')\n",
    "        if i < len(jobs) - 1:\n",
    "            print(\"    ),\")\n",
    "        else:\n",
    "            print(\"    )\")\n",
    "    print(\"]\")\n",
    "\n",
    "# Create jobs list\n",
    "jobs = create_jobs_from_missing_folders(all_folders_with_missing_files)\n",
    "\n",
    "# Print in the required format\n",
    "print_jobs_list(jobs)\n",
    "\n",
    "# Copy and paste the output into your script run_missing_data.py and run it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22cbbc",
   "metadata": {},
   "source": [
    "# Copy pasting the processed folders into output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb43c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def copy_processed_folders_simple(source_path, destination_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Alternative version that copies all 'processed_' folders into a subfolder\n",
    "    named after the source folder within the destination.\n",
    "    \n",
    "    Args:\n",
    "        source_path (str): The path to search for 'processed_' folders\n",
    "        destination_path (str): The path where the folders should be copied to\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples (source_path, destination_path) for each copied folder\n",
    "    \"\"\"\n",
    "    copied_folders = []\n",
    "    \n",
    "    # Check if source path exists\n",
    "    if not os.path.exists(source_path):\n",
    "        print(f\"Error: Source path '{source_path}' does not exist\")\n",
    "        return copied_folders\n",
    "    \n",
    "    # Get the name of the source folder\n",
    "    source_folder_name = os.path.basename(os.path.abspath(source_path))\n",
    "    \n",
    "    # Create the destination structure: destination_path/source_folder_name/\n",
    "    final_destination = os.path.join(destination_path, source_folder_name)\n",
    "    \n",
    "    # Create destination directory if it doesn't exist\n",
    "    if not os.path.exists(final_destination):\n",
    "        os.makedirs(final_destination)\n",
    "        print(f\"Created destination directory: {final_destination}\")\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        # Check each directory name\n",
    "        for dir_name in dirs:\n",
    "            if dir_name.startswith('processed_'):\n",
    "                source_folder = os.path.join(root, dir_name)\n",
    "                dest_folder = os.path.join(final_destination, dir_name)\n",
    "                \n",
    "                # Handle name conflicts by adding a number suffix\n",
    "                counter = 1\n",
    "                original_dest = dest_folder\n",
    "                while os.path.exists(dest_folder):\n",
    "                    dest_folder = f\"{original_dest}_{counter}\"\n",
    "                    counter += 1\n",
    "                \n",
    "                try:\n",
    "                    # Copy the entire folder and its contents\n",
    "                    shutil.copytree(source_folder, dest_folder)\n",
    "                    copied_folders.append((source_folder, dest_folder))\n",
    "                    print(f\"Copied: {source_folder} -> {dest_folder}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error copying {source_folder}: {str(e)}\")\n",
    "    \n",
    "    return copied_folders\n",
    "\n",
    "source_folder = folder_path\n",
    "dest_folder = \"/home/kaibald231/ABC_Processed\"  # Change this to your desired destination folder\n",
    "\n",
    "# Use the main function (preserves directory structure)\n",
    "result = copy_processed_folders_simple(source_folder, dest_folder)\n",
    "\n",
    "print(f\"\\nCopied {len(result)} 'processed_' folders:\")\n",
    "for src, dst in result:\n",
    "    print(f\"  {src} -> {dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20526b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pupil_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
